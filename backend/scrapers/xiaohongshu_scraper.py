
import asyncio
import random
from typing import List, Dict, Any
from loguru import logger
from config import settings
# from spider_xhs.apis.xhs_pc_apis import XHS_Apis # Deprecated API
# from spider_xhs.xhs_utils.data_util import handle_note_info
from scrapers.xhs_browser import xhs_browser_scraper

class XiaohongshuScraper:
    """å°çº¢ä¹¦ç¬”è®°æŠ“å–å™¨ï¼ˆAPI çˆ¬è™«ç‰ˆ -> æµè§ˆå™¨è‡ªåŠ¨åŒ–ç‰ˆï¼‰"""
    
    def __init__(self):
        self.platform = "xiaohongshu"
        # self.xhs_apis = XHS_Apis() # Deprecated
    
    async def search(self, keyword: str, max_results: int = 10) -> List[Dict[str, Any]]:
        """æœç´¢ç¬”è®°å¹¶æå–åœ°ç‚¹ä¿¡æ¯ (ä½¿ç”¨ Playwright æµè§ˆå™¨)"""
        logger.info(f"ğŸ” XHS Browser æœç´¢: {keyword}, æœ€å¤š {max_results} æ¡")
        
        try:
            # ä½¿ç”¨æµè§ˆå™¨çˆ¬è™«æœç´¢
            browser_results = await xhs_browser_scraper.search(keyword, max_results)
            
            results = []
            for item in browser_results:
                # è½¬æ¢æ ¼å¼
                converted = self._convert_to_place_info(item)
                if converted:
                    results.append(converted)
            
            logger.info(f"ğŸ“ å…±è·å– {len(results)} æ¡æœ‰æ•ˆç»“æœ")
            return results
            
        except Exception as e:
            logger.error(f"å°çº¢ä¹¦æœç´¢å¤±è´¥: {e}")
            return []

    def _convert_to_place_info(self, note_info: dict) -> Dict[str, Any]:
        """å°†çˆ¬è™«è·å–çš„ç¬”è®°ä¿¡æ¯è½¬æ¢ä¸º DayTripPlanner çš„åœ°ç‚¹æ ¼å¼"""
        title = note_info.get('title', '')
        # Browser scraper might not get full desc/content without clicking
        desc = note_info.get('desc', '') or title 
        
        if not title:
            return None
        
        # ä»æ ‡é¢˜æå–åœ°ç‚¹åç§°
        place_name = self._extract_place_name(title)
        
        # çŒœæµ‹ç±»åˆ«
        category = self._guess_category(title + " " + desc)
        
        # è§£æäº’åŠ¨æ•°æ®
        likes = self._safe_int(note_info.get('likes', 0))
        
        return {
            "name": place_name,
            "title": title,
            "description": desc[:500],
            "likes": likes,
            "collected": 0, # Browser list view might not show this
            "comments": 0,  # Browser list view might not show this
            "note_url": note_info.get('note_url', ''),
            "category": category,
            "source": "xiaohongshu",
            "tags": [],
            "images": [], # To do: extract images in browser scraper
            "author": note_info.get('author', ''),
            "upload_time": "",
        }
    
    def _safe_int(self, value) -> int:
        """å®‰å…¨è½¬æ¢ä¸ºæ•´æ•°"""
        if isinstance(value, int):
            return value
        if isinstance(value, str):
            text = value.strip()
            if not text:
                return 0
            try:
                import re
                if "ä¸‡" in text:
                    return int(float(text.replace("ä¸‡", "")) * 10000)
                elif "k" in text.lower():
                    return int(float(text.lower().replace("k", "")) * 1000)
                else:
                    return int(re.sub(r"[^\d]", "", text) or 0)
            except:
                return 0
        return 0
    
    def _guess_category(self, text: str) -> str:
        """æ ¹æ®æ–‡æœ¬çŒœæµ‹ç±»åˆ«"""
        food_keywords = ["ç¾é£Ÿ", "é¤å…", "å¥½åƒ", "åƒé¥­", "ç«é”…", "å’–å•¡", "ç”œå“", "å°åƒ", "é¢é¦†", "é¥­åº—", "çƒ§çƒ¤", "å¥¶èŒ¶"]
        attraction_keywords = ["æ™¯ç‚¹", "æ‰“å¡", "æ‹ç…§", "é£æ™¯", "å…¬å›­", "å¤é•‡", "åšç‰©é¦†", "å¯ºåº™", "æ¹–", "å±±", "å¤åŸ", "å¤œæ™¯"]
        shopping_keywords = ["è´­ç‰©", "å•†åœº", "å¸‚é›†", "ä¹°", "ç‰¹äº§", "æ­¥è¡Œè¡—"]
        
        for kw in food_keywords:
            if kw in text:
                return "ç¾é£Ÿ"
        for kw in attraction_keywords:
            if kw in text:
                return "æ™¯ç‚¹"
        for kw in shopping_keywords:
            if kw in text:
                return "è´­ç‰©"
        
        return "æ™¯ç‚¹"  # é»˜è®¤
    
    def _extract_place_name(self, title: str) -> str:
        """ä»æ ‡é¢˜æå–åœ°ç‚¹åç§°"""
        # ç®€å•å¤„ç†ï¼šå–|æˆ–ï½œå‰çš„éƒ¨åˆ†
        for sep in ["|", "ï½œ", "â€”", "-", "Â·"]:
            if sep in title:
                title = title.split(sep)[0].strip()
                break
        
        # ç§»é™¤å¸¸è§å‰ç¼€
        prefixes = ["æ¨è", "å¿…å»", "æ¢åº—", "æ‰“å¡"]
        for prefix in prefixes:
            title = title.replace(prefix, "")
        
        return title.strip()[:30]  # é™åˆ¶é•¿åº¦

# å…¨å±€å®ä¾‹
xiaohongshu_scraper = XiaohongshuScraper()
